# 🤖 LLM Quantized Evaluator

Una app interactiva desarrollada con **Streamlit** para evaluar y comparar modelos de lenguaje grandes (LLMs) en formato **GGUF** usando **llama.cpp**, con enfoque en ejecución sobre CPU en entornos de recursos limitados.

El proyecto mide:

- ⏱️ Latencia de inferencia
- 🎯 Precisión semántica (comparación contra respuestas esperadas)
- 🧠 Cobertura del prompt
- 📏 Longitud de la respuesta

---

## 🚀 ¿Qué modelos se evalúan?

- **Phi-2**
- **TinyLlama 1.1B**
- **Llama-2 7B**
- **Mistral 7B (versión liviana Q4_0)**

Todos los modelos se descargan automáticamente desde Hugging Face si no están presentes.

---

## 📦 Estructura esperada del proyecto

