#  LLM Quantized Evaluator

Una app interactiva desarrollada con **Streamlit** para evaluar y comparar modelos de lenguaje grandes (LLMs) en formato **GGUF** usando **llama.cpp**, con enfoque en ejecuci贸n sobre CPU en entornos de recursos limitados.

El proyecto mide:

- 憋 Latencia de inferencia
-  Precisi贸n sem谩ntica (comparaci贸n contra respuestas esperadas)
-  Cobertura del prompt
-  Longitud de la respuesta

---

##  驴Qu茅 modelos se eval煤an?

- **Phi-2**
- **TinyLlama 1.1B**
- **Llama-2 7B**
- **Mistral 7B (versi贸n liviana Q4_0)**

Todos los modelos se descargan autom谩ticamente desde Hugging Face si no est谩n presentes.

---

##  Estructura esperada del proyecto

